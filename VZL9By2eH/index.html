<!DOCTYPE html>
<html>
<head>
<link rel="shortcut icon" href="https://18279493170.github.io/favicon.ico" type="image/x-icon" /><meta name="viewport"content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"/><meta name="apple-mobile-web-app-capable"content="yes"/><meta name="apple-mobile-web-app-status-bar-style"content="black"/><meta name="format-detection"content="telephone=no"/><meta name="renderer"content="webkit"><meta name="description"content="搞钱！搞钱！搞钱！"><meta charset="UTF-8"><title>python爬虫 | 徐某人</title>
<link href="https://18279493170.github.io/styles/main.css" type="text/css" rel="stylesheet" /><link href="https://at.alicdn.com/t/font_1621793_zatzzgvf30g.css" type="text/css" rel="stylesheet" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.css"><script async src="https://cdn.jsdelivr.net/npm/busuanzi@2.3.0/bsz.pure.mini.min.js"></script><script src="https://18279493170.github.io/media/js/magnify.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
<script type="text/javascript">function btn_toggle(){document.getElementById("hn").classList.contains("no-js")?document.getElementById("hn").classList.remove("no-js"):document.getElementById("hn").classList.add("no-js")}</script>

<link rel="canonical" href="https://18279493170.github.io/VZL9By2eH/" />
</head>
<body>
<div class="progress"></div><style>.progress{background:linear-gradient(to right,#87ceeb var(--scroll),transparent 0);background-repeat:no-repeat;position:fixed;width:100%;height:4px;z-index:1}</style><div class="darkmode-background"></div><div class="darkmode-layer"></div>
<noscript><p class="warn" >本页面需要浏览器支持（启用）JavaScript</p></noscript><div class="header"><div class="logo_title"><div class="title animated fadeInDown"><a href="https://18279493170.github.io"><img alt="logo" style="display:inline-block;" src="https://18279493170.github.io/images/avatar.png"/></a><h1 title="徐某人" class="weaklink"><a  href="/">徐某人</a></h1>

<div class="navbar weaklink">
<div class="normal_nav">
<div class="bitcron_nav_container"><div class="bitcron_nav"><div class="bitcron_nav"><div style="display:flex;justify-content:center;"><nav class="mixed_site_nav_wrap site_nav_wrap"><ul class="mixed_site_nav site_nav sm sm-base">	<li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/" class="selected active current nav__item" >首页</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/archives" class="selected active current nav__item" >归档</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/tags" class="selected active current nav__item" >标签</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="https://18279493170.github.io/ju-ji" class="selected active current nav__item" >句集</a></li></ul></nav>
<div style="float:right;margin-top:1em"><form id="gridea-search-form" data-update="1578893743252" action="/search/index.html"><input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="Search..."></form></div><div style="margin-left:0.5em;margin-top:1.2em"><input id="switch_default" onclick="mobileBtn()" type="checkbox" class="switch_default"><label for="switch_default" class="toggleBtn"></label></div></div>
<div class="clear clear_nav_inline_end"></div></div></div><div class="clear clear_nav_end"></div></div></div><div class="hamberger" href="javascript:void(0)" onclick="btn_toggle();"><i class="iconfont icon-category"></i></div></div></div></div>
<div id="hn" class="no-js hidden_nav animated fadeInDown"><div class="bitcron_nav_container"><div class="bitcron_nav"><nav class="mixed_site_nav_wrap site_nav_wrap"><ul class="mixed_site_nav site_nav sm sm-base">	<li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/" class="selected active current nav__item" >首页</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/archives" class="selected active current nav__item" >归档</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/tags" class="selected active current nav__item" >标签</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="https://18279493170.github.io/ju-ji" class="selected active current nav__item" >句集</a></li></ul><div class="clear clear_nav_inline_end"></div></nav></div><div class="clear clear_nav_end"></div></div>
<div style="display:flex;justify-content:center;inline-block;text-align:center;margin-top:7%"><div><form id="gridea-search-form" data-update="1677698765253" action="/search/index.html"><input class="search-input" autocomplete="off" spellcheck="false" name="q"  placeholder="Search..." /></form></div><div style="margin-left:0.5em"><input id="switch_default_h" onclick="mobileBtn()" type="checkbox" class="switch_default"><label for="switch_default" class="toggleBtn"></label></div></div>
</div></div>
<script>function enableDarkmode(){document.body.classList.add("darkmode"),document.getElementById("switch_default").checked=1,document.getElementById("switch_default_h").checked=1}function removeDarkmode(){document.body.classList.remove("darkmode"),document.getElementById("switch_default").checked=0,document.getElementById("switch_default_h").checked=0}function getCookie(a){var b,c=new RegExp("(^| )"+a+"=([^;]*)(;|$)");return(b=document.cookie.match(c))?unescape(b[2]):null}cookie=getCookie("darkmode"),"enable"==cookie&&enableDarkmode(),window.matchMedia("(prefers-color-scheme: dark)").matches&&"disable"!==cookie&&(enableDarkmode(),document.cookie="darkmode=enable; path=/");var mobileBtn=function(){1==document.getElementById("switch_default").checked?(enableDarkmode(),document.cookie="darkmode=enable; path=/"):(removeDarkmode(),document.cookie="darkmode=disable; path=/")};</script>

<div class="main"><div class="main-inner"><div class="content">
<article class="post">
<h2 class="post_title sm_margin"><a>python爬虫</a></h2>
<script>function lan(){if(document.getElementById("lan").innerText=="繁"){var s=document.getElementById("tongwenlet_cn");if(s!=null){document.body.removeChild(s)}var s=document.createElement("script");s.language="javascript";s.type="text/javascript";s.src="https://cdn.jsdelivr.net/gh/qyxtim/Static@1.1/bookmarklet_tw.js";s.id="tongwenlet_cn";document.body.appendChild(s);document.getElementById("lan").innerHTML="简"}else{if(document.getElementById("lan").innerText=="簡"){var s=document.getElementById("tongwenlet_cn");if(s!=null){document.body.removeChild(s)}var s=document.createElement("script");s.language="javascript";s.type="text/javascript";s.src="https://cdn.jsdelivr.net/gh/qyxtim/Static@1.1/bookmarklet_cn.js";s.id="tongwenlet_cn";document.body.appendChild(s);document.getElementById("lan").innerHTML="繁"}}};</script>
<section class="post_details"><i class="iconfont icon-calendar"></i><span style="margin-right:15px"> 2022-06-07</span><i class="iconfont icon-browse"></i><span style="margin-right:15px"> <span id="busuanzi_value_page_pv"></span> Views</span><i class="iconfont icon-category"></i><span class="weaklink" style="margin-right:15px">	<a href="https://18279493170.github.io/GRlmjsjZsM/" class="tag">python</a></span><i class="iconfont icon-caret-down"></i><span style="margin-right:15px">11219字</span><i class="iconfont icon-naozhong"></i><span style="margin-right:15px">47 min read</span><a id="lan" href="javascript:void(0);"onclick="lan();"title="调整简繁体" style="margin-right:15px;">繁</a>
</section>

<div style="display:flex">
<div class="md_block" id="md_block">
<div class="round-shape-one"></div>
<p>1、通用爬虫与聚焦爬虫<br>
通用网络爬虫 是 捜索引擎抓取系统（Baidu、Google、Yahoo等）的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。<br>
聚焦爬虫<br>
聚焦爬虫，是&quot;面向特定主题需求&quot;的一种网络爬虫程序，它与通用搜索引擎爬虫的区别在于： 聚焦爬虫在实施网页抓取时会对内容进行处理筛选，尽量保证只抓取与需求相关的网页信息。</p>
<p>2、浏览器发送HTTP请求的过程：<br>
当用户在浏览器的地址栏中输入一个URL地址并按回车键之后，浏览器会向HTTP服务器发送HTTP请求。</p>
<pre><code>HTTP请求主要分为 Get 和 Post 两种方法。

当我们在浏览器输入URL http://www.baidu.com 的时候，浏览器发送一个Request请求去获取 http://www.baidu.com 的html文件，服务器把Response文件对象发送回给浏览器。

浏览器分析Response中的 HTML，发现其中引用了很多其他文件，比如Images文件，CSS文件，JS文件。 浏览器会自动再次发送Request去获取图片，CSS文件，或者JS文件等。

当所有的文件都下载成功后，网页会根据HTML语法结构，完整的显示出来了
</code></pre>
<p>3、URL基本格式<br>
基本格式：scheme://host[:port]/path/…/[?query-string][#anchor]<br>
#锚点</p>
<pre><code>scheme：协议(例如：http, https, ftp)
host：服务器的IP地址或者域名
port：服务器的端口（如果是走协议默认端口，缺省端口80）
path：访问资源的路径
query-string：参数，发送给http服务器的数据
anchor：锚（跳转到网页的指定锚点位置）
</code></pre>
<p>4、URL只是标识资源的位置，而HTTP是用来提交和获取资源。客户端发送一个HTTP请求到服务器的请求消息，包括以下格式：</p>
<pre><code>4、1请求行、请求头部、空行、请求数据
图标见同级文件夹

例子：
GET https://www.baidu.com/ HTTP/1.1
Host: www.baidu.com
Connection: keep-alive
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
Referer: http://www.baidu.com/
Accept-Encoding: gzip, deflate, sdch, br
Accept-Language: zh-CN,zh;q=0.8,en;q=0.6
Cookie: BAIDUID=04E4001F34EA74AD4601512DD3C41A7B:FG=1; BIDUPSID=04E4001F34EA74AD4601512DD3C41A7B; PSTM=1470329258; MCITY=-343%3A340%3A; H_PS_PSSID=1447_18240_21105_21386_21454_21409_21554; BD_UPN=12314753; sug=3; sugstore=0; ORIGIN=0; bdime=0; H_PS_645EC=7e2ad3QHl181NSPbFbd7PRUCE1LlufzxrcFmwYin0E6b%2BW8bbTMKHZbDP0g; BDSVRTM=0

4、2常用的请求报头

    1. Host (主机和端口号)
    Host：对应网址URL中的Web名称和端口号，用于指定被请求资源的Internet主机和端口号，通常属于URL的Host部分。

    2. Connection (连接类型)
    Connection：表示客户端与服务连接类型，通常情况下：

    Client 发起一个包含 Connection:keep-alive 的请求（HTTP/1.1使用 keep-alive 为默认值）

    Server收到请求后：

    如果 Server 支持 keep-alive，回复一个包含 Connection:keep-alive 的响应，不关闭连接；
    如果 Server 不支持 keep-alive，回复一个包含 Connection:close 的响应，关闭连接。
    如果client收到包含 Connection:keep-alive 的响应，向同一个连接发送下一个请求，直到一方主动关闭连接。

    Connection : keep-alive 在很多情况下能够重用连接，减少资源消耗，缩短响应时间。比如当浏览器需要多个文件时(比如一个HTML文件和多个Image文件)，不需要每次都去请求建立连接。

    3. Upgrade-Insecure-Requests (升级为HTTPS请求)
    Upgrade-Insecure-Requests：升级不安全的请求，意思是会在加载 http 资源时自动替换成 https 请求，让浏览器不再显示https页面中的http请求警报。

    HTTPS 是以安全为目标的 HTTP 通道，所以在 HTTPS 承载的页面上不允许出现 HTTP 请求，一旦出现就是提示或报错。

    4. User-Agent (浏览器名称)
    User-Agent：标识客户端身份的名称，通常页面会根据不同的User-Agent信息自动做出适配，甚至返回不同的响应内容。

    5. Accept (传输文件类型)
    Accept：指浏览器或其他客户端可以接受的MIME（Multipurpose Internet Mail Extensions（多用途互联网邮件扩展））文件类型，服务器可以根据它判断并返回适当的文件格式。

    举例：
    Accept: */*：表示什么都可以接收。

    Accept：image/gif：表明客户端希望接受GIF图像格式的资源；

    Accept：text/html：表明客户端希望接受html文本。

    Accept: text/html, application/xhtml+xml;q=0.9, image/*;q=0.8：表示浏览器支持的 MIME 类型分别是 html文本、xhtml和xml文档、所有的图像格式资源。

    q是权重系数，范围 0 =&lt; q &lt;= 1，q 值越大，请求越倾向于获得其“;”之前的类型表示的内容。若没有指定q值，则默认为1，按从左到右排序顺序；若被赋值为0，则用于表示浏览器不接受此内容类型。

    Text：用于标准化地表示的文本信息，文本消息可以是多种字符集和或者多种格式的；Application用于传输应用程序数据或者二进制数据。详细请点击

    6. Referer (页面跳转来源)
    Referer：表明产生请求的网页来自于哪个URL，用户是从该 Referer页面访问到当前请求的页面。这个属性可以用来跟踪Web请求来自哪个页面，是从什么网站来的等。

    防盗链：有时候遇到下载某网站图片，需要对应的referer，否则无法下载图片，那是因为人家做了防盗链，原理就是根据referer去判断是否是本网站的地址，如果不是，则拒绝，如果是，就可以下载。

    7. Accept-Encoding（文件编解码格式）
    Accept-Encoding：指出浏览器可以接受的编码方式。编码方式不同于文件格式，它是为了压缩文件并加速文件传递速度。浏览器在接收到Web响应之后先解码，然后再检查文件格式，许多情形下这可以减少大量的下载时间。

    举例：Accept-Encoding:gzip;q=1.0, identity; q=0.5, *;q=0
    如果有多个Encoding同时匹配, 按照q值顺序排列，本例中按顺序支持 gzip, identity压缩编码，支持gzip的浏览器会返回经过gzip编码的HTML页面。

    如果请求消息中没有设置这个报头，通常服务器假定客户端不支持压缩，直接返回文本。

    8. Accept-Language（语言种类）
    Accept-Langeuage：指出浏览器可以接受的语言种类，如en或en-us指英语，zh或者zh-cn指中文，当服务器能够提供一种以上的语言版本时要用到。

    如果目标网站支持多个语种的话，可以使用这个信息来决定返回什么语言的网页。

    9. Accept-Charset（字符编码）
    Accept-Charset：指出浏览器可以接受的字符编码。

    举例：Accept-Charset:iso-8859-1,gb2312,utf-8
    ISO8859-1：通常叫做Latin-1。Latin-1包括了书写所有西方欧洲语言不可缺少的附加字符，英文浏览器的默认值是ISO-8859-1.
    gb2312：标准简体中文字符集;
    utf-8：UNICODE 的一种变长字符编码，可以解决多种语言文本显示问题，从而实现应用国际化和本地化。
    如果在请求消息中没有设置这个域，默认客户端是任何字符集都可以接受，则返回网页charset指定的编码。

    10. Cookie （Cookie）
    Cookie：浏览器用这个属性向服务器发送Cookie。Cookie是在浏览器中寄存的小型数据体，它可以记载和服务器相关的用户信息，也可以用来实现模拟登陆，之后会详细讲。

    11. Content-Type (POST数据类型)
    Content-Type：POST请求里用来表示的内容类型。

    举例：Content-Type = Text/XML; charset=gb2312：
    指明该请求的消息体中包含的是纯文本的XML类型的数据，字符编码采用“gb2312”。


4.3 常用的响应报头

    1. Cache-Control：must-revalidate, no-cache, private。
    这个值告诉客户端，服务端不希望客户端缓存资源，在下次请求资源时，必须要从新请求服务器，不能从缓存副本中获取资源。

    Cache-Control是响应头中很重要的信息，当客户端请求头中包含Cache-Control:max-age=0请求，明确表示不会缓存服务器资源时,Cache-Control作为作为回应信息，通常会返回no-cache，意思就是说，&quot;那就不缓存呗&quot;。

    当客户端在请求头中没有包含Cache-Control时，服务端往往会定,不同的资源不同的缓存策略，比如说oschina在缓存图片资源的策略就是Cache-Control：max-age=86400,这个意思是，从当前时间开始，在86400秒的时间内，客户端可以直接从缓存副本中读取资源，而不需要向服务器请求。

    2. Connection：keep-alive
    这个字段作为回应客户端的Connection：keep-alive，告诉客户端服务器的tcp连接也是一个长连接，客户端可以继续使用这个tcp连接发送http请求。

    3. Content-Encoding:gzip
    告诉客户端，服务端发送的资源是采用gzip编码的，客户端看到这个信息后，应该采用gzip对资源进行解码。

    4. Content-Type：text/html;charset=UTF-8
    告诉客户端，资源文件的类型，还有字符编码，客户端通过utf-8对资源进行解码，然后对资源进行html解析。通常我们会看到有些网站是乱码的，往往就是服务器端没有返回正确的编码。

    5. Date：Sun, 21 Sep 2016 06:18:21 GMT
    这个是服务端发送资源时的服务器时间，GMT是格林尼治所在地的标准时间。http协议中发送的时间都是GMT的，这主要是解决在互联网上，不同时区在相互请求资源的时候，时间混乱问题。

    6. Expires:Sun, 1 Jan 2000 01:00:00 GMT
    这个响应头也是跟缓存有关的，告诉客户端在这个时间前，可以直接访问缓存副本，很显然这个值会存在问题，因为客户端和服务器的时间不一定会都是相同的，如果时间不同就会导致问题。所以这个响应头是没有Cache-Control：max-age=*这个响应头准确的，因为max-age=date中的date是个相对时间，不仅更好理解，也更准确。

    7. Pragma:no-cache
    这个含义与Cache-Control等同。

    8.Server：Tengine/1.4.6
    这个是服务器和相对应的版本，只是告诉客户端服务器的信息。

    9. Transfer-Encoding：chunked
    这个响应头告诉客户端，服务器发送的资源的方式是分块发送的。一般分块发送的资源都是服务器动态生成的，在发送时还不知道发送资源的大小，所以采用分块发送，每一块都是独立的，独立的块都能标示自己的长度，最后一块是0长度的，当客户端读到这个0长度的块时，就可以确定资源已经传输完了。

    10. Vary: Accept-Encoding
    告诉缓存服务器，缓存压缩文件和非压缩文件两个版本，现在这个字段用处并不大，因为现在的浏览器都是支持压缩的。
</code></pre>
<p>5、请求方式<br>
1    GET	请求指定的页面信息，并返回实体主体。<br>
2	POST	向指定资源提交数据进行处理请求（例如提交表单或者上传文件），数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。<br>
3	HEAD	类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头<br>
4	PUT	从客户端向服务器传送的数据取代指定的文档的内容。<br>
5	DELETE	请求服务器删除指定的页面。<br>
6	CONNECT	HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。<br>
7	OPTIONS	允许客户端查看服务器的性能。<br>
8	TRACE	回显服务器收到的请求，主要用于测试或诊断。</p>
<p>6、主要的请求方式以及对比，特别重要<br>
HTTP请求主要分为Get和Post两类：<br>
GET是从服务器上获取指定页面信息，POST是向服务器提交数据并获取页面信息。</p>
<pre><code>GET请求参数都显示在URL上，服务器根据该请求所包含URL中的参数来产生响应内容。 &quot;Get&quot; 请求的参数 是URL的一部分。

POST请求参数在请求体当中，消息长度没有限制而且以隐式的方式进行发送，通常用来向HTTP服务器提交量比较大的数据（比如请求中包含许多参数或者文件上传操作等）。 &quot;POST&quot;请求的参数 不在URL中，而在请求体中。

页面的form表单一般都有method属性，默认值是&quot;get&quot;。 举个栗子，登录时提交用户名和密码：

如果用&quot;get&quot;方式，提交表单后，则用户输入的用户名和密码将在地址栏中暴露无遗；

如果设置为&quot;post，则提交表单后，地址栏不会有用户名和密码的显示。

所以处理登录页面的form表单时，发送的请求都是&quot;POST&quot;方式。
</code></pre>
<p>7、常见的响应状态码<br>
100~199：表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。</p>
<pre><code>200~299：表示服务器成功接收请求并已完成整个处理过程。常用200（OK 请求成功）。

300~399：为完成请求，客户需进一步细化请求。例如：请求的资源已经移动一个新地址、常用302（所请求的页面已经临时转移至新的url）、307和304（使用缓存资源）。
400~499：客户端的请求有错误，常用404（服务器无法找到被请求的页面）、403（服务器拒绝访问，权限不够）。
500~599：服务器端出现错误，常用500（请求未完成。服务器遇到不可预知的情况）。
</code></pre>
<p>8、HTTP代理器FIDDLER</p>
<pre><code>https://blog.csdn.net/qq_32252917/article/details/79074180

Headers —— 显示客户端发送到服务器的 HTTP 请求的 header，显示为一个分级视图，包含了 Web 客户端信息、Cookie、传输状态等。
Textview —— 显示 POST 请求的 body 部分为文本。
WebForms —— 显示请求的 GET 参数 和 POST body 内容。
HexView —— 用十六进制数据显示请求。
Auth —— 显示响应 header 中的 Proxy-Authorization(代理身份验证) 和 Authorization(授权) 信息.
Raw —— 将整个请求显示为纯文本。
JSON - 显示JSON格式文件。
XML —— 如果请求的 body 是 XML 格式，就是用分级的 XML 树来显示它。


Transformer —— 显示响应的编码信息。
Headers —— 用分级视图显示响应的 header。
TextView —— 使用文本显示相应的 body。
ImageVies —— 如果请求是图片资源，显示响应的图片。
HexView —— 用十六进制数据显示响应。
WebView —— 响应在 Web 浏览器中的预览效果。
Auth —— 显示响应 header 中的 Proxy-Authorization(代理身份验证) 和 Authorization(授权) 信息。
Caching —— 显示此请求的缓存信息。
Privacy —— 显示此请求的私密 (P3P) 信息。
Raw —— 将整个响应显示为纯文本。
JSON - 显示JSON格式文件。
XML —— 如果响应的 body 是 XML 格式，就是用分级的 XML 树来显示它 。
</code></pre>
<p>9、Python2 与Python3的对比<br>
urllib2  Python2.7 自带的模块(不需要下载，导入即可使用)<br>
urllib.request python3中用</p>
<p>10、urlopen 的使用<br>
# urllib2_urlopen.py</p>
<pre><code># 导入urllib2 库
import urllib2

# 向指定的url发送请求，并返回服务器响应的类文件对象
response = urllib2.urlopen(&quot;http://www.baidu.com&quot;)

# 类文件对象支持 文件对象的操作方法，如read()方法读取文件全部内容，返回字符串
html = response.read()

# 打印字符串
print html
</code></pre>
<p>11、隐藏身份，增加HTTP报头，创造请求实例</p>
<pre><code>11、1 参数的学习


data（默认空）：提交的Form表单数据，同时 HTTP 请求方法将从默认的 &quot;GET&quot;方式 改为 &quot;POST&quot;方式。

headers（默认空）：参数为字典类型，包含了需要发送的HTTP报头的键值对。


11、2 实例
    import urllib2

    # url 作为Request()方法的参数，构造并返回一个Request对象
    request = urllib2.Request(&quot;http://www.baidu.com&quot;)

    # Request对象作为urlopen()方法的参数，发送给服务器并接收响应
    response = urllib2.urlopen(request)

    html = response.read()

    print html

11、3 模拟浏览器访问 

    import urllib2

    url = &quot;http://www'baidu.cn&quot;

    # IE 9.0 的 User-Agent，包含在 user_agent里
    user_agent = {&quot;User-Agent&quot; : &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)&quot;} 

    #  url 连同 headers，一起构造Request请求，这个请求将附带 IE9.0 浏览器的User-Agent
    request = urllib2.Request(url, headers = user_agent)

    # 向服务器发送这个请求
    response = urllib2.urlopen(request)

    html = response.read()
    print html

11、4 增加更多的请求头信息
    user_agent = {&quot;User-Agent&quot; : &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)&quot;} 
    request = urllib2.Request(url, headers = user_agent)

    #也可以通过调用Request.add_header() 添加/修改一个特定的header
    request.add_header(&quot;Connection&quot;, &quot;keep-alive&quot;)

    # 也可以通过调用Request.get_header()来查看header信息
    # request.get_header(header_name=&quot;Connection&quot;)


    查看响应状态码

    print response.code     #可以查看响应状态码


11.5 实例
    import urllib2
    import random

    url = &quot;http://www.baidu.com&quot;

    ua_list = [
        &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1&quot;,
        &quot;Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11&quot;,
        &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6&quot;,
        &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6&quot;
    ]

    user_agent = random.choice(ua_list)

    request = urllib2.Request(url)

    #也可以通过调用Request.add_header() 添加/修改一个特定的header
    request.add_header(&quot;User-Agent&quot;, user_agent)

    # get_header()的字符串参数，第一个字母大写，后面的全部小写
    request.get_header(&quot;User-agent&quot;)

    response = urllib2.urlopen(request)

    html = response.read()
    print html
</code></pre>
<p>12、URL编码的问题<br>
urllib与urllib2 的对比<br>
urllib 模块仅可以接受URL，不能创建 设置了headers 的Request 类实例<br>
urlencode 方法用来产生GET查询字符串，而 urllib2 则没有，所以搭配使用<br>
查询紫川解码  unquote()</p>
<pre><code>实例：

    import urllib

    word = {&quot;kw&quot; : &quot;百度&quot;}

    # 通过urllib.urlencode()方法，将字典键值对按URL编码转换，从而能被web服务器接受。
    urllib.urlencode(word)  
   

    # 通过urllib.unquote()方法，把 URL编码字符串，转换回原先字符串。
    In [4]: print urllib.unquote(&quot;wd=zfds&quot;)


总结：一般HTTP请求提交数据，需要编码成 URL编码格式，然后做为url的一部分，或者作为参数传到Request对象中
</code></pre>
<p>13、urllib urllib2结合使用</p>
<pre><code>13、1 直接获取网页
        def tiebaSpider(url, beginPage, endPage):
      

            for page in range(beginPage, endPage + 1):
                pn = (page - 1) * 50

                filename = &quot;第&quot; + str(page) + &quot;页.html&quot;
                fullurl = url + &quot;&amp;pn=&quot; + str(pn)

                # 调用loadPage()发送请求获取HTML页面
                html = loadPage(fullurl, filename)
                # 将获取到的HTML页面写入本地磁盘文件
                writeFile(html, filename)

        def loadPage(url, filename):
        
            print &quot;正在下载&quot; + filename

            headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;&quot;}

            request = urllib2.Request(url, headers = headers)
            response = urllib2.urlopen(request)
            return response.read()


        def writeFile(html, filename):
         
            print &quot;正在存储&quot; + filename
            with open(filename, 'w') as f:
                f.write(html)
            print &quot;-&quot; * 20



        if __name__ == &quot;__main__&quot;:

            kw = raw_input(&quot;请输入需要爬取的贴吧:&quot;)
            # 输入起始页和终止页，str转成int类型
            beginPage = int(raw_input(&quot;请输入起始页：&quot;))
            endPage = int(raw_input(&quot;请输入终止页：&quot;))

            url = &quot;http://tieba.baidu.com/f?&quot;
            key = urllib.urlencode({&quot;kw&quot; : kw})

            # 组合后的url示例：http://tieba.baidu.com/f?kw=lol
            url = url + key
            tiebaSpider(url, beginPage, endPage)
            
13、2 ajax 抓取
    AJAX请求一般返回给网页的是JSON文件，只要对AJAX请求地址进行POST或GET，就能返回JSON数据
    不能直接对网页url获取


    13、2、1 第一个例子
            url = &quot;https://movie.douban.com/j/chart/top_list?type=11&amp;interval_id=100%3A90&amp;action=&amp;&quot;

            headers={&quot;User-Agent&quot;: &quot;Mozilla....&quot;}

            # 变动的是这两个参数，从start开始往后显示limit个
            formdata = {
                'start':'0',
                'limit':'10'
            }
            data = urllib.urlencode(formdata)

            request = urllib2.Request(url + data, headers = headers)
            response = urllib2.urlopen(request)

            print response.read()


            # demo2

            url = &quot;https://movie.douban.com/j/chart/top_list?&quot;
            headers={&quot;User-Agent&quot;: &quot;Mozilla....&quot;}

            # 处理所有参数
            formdata = {
                'type':'11',
                'interval_id':'100:90',
                'action':'',
                'start':'0',
                'limit':'10'
            }
            data = urllib.urlencode(formdata)

            request = urllib2.Request(url + data, headers = headers)
            response = urllib2.urlopen(request)

            print response.read()

    13、2、2 发送post 请求

            import urllib
            import urllib2

            # POST请求的目标URL
            url = &quot;http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;smartresult=ugc&amp;sessionFrom=null&quot;

            headers={&quot;User-Agent&quot;: &quot;Mozilla....&quot;}

            formdata = {
                &quot;type&quot;:&quot;AUTO&quot;,
                &quot;i&quot;:&quot;i love python&quot;,
                &quot;doctype&quot;:&quot;json&quot;,
                &quot;xmlVersion&quot;:&quot;1.8&quot;,
                &quot;keyfrom&quot;:&quot;fanyi.web&quot;,
                &quot;ue&quot;:&quot;UTF-8&quot;,
                &quot;action&quot;:&quot;FY_BY_ENTER&quot;,
                &quot;typoResult&quot;:&quot;true&quot;
            }

            data = urllib.urlencode(formdata)

            request = urllib2.Request(url, data = data, headers = headers)
            response = urllib2.urlopen(request)
            print response.read()
</code></pre>
<p>14、HTTP 和HTTPS 的区别</p>
<pre><code>urllib2可以为 HTTPS 请求验证SSL证书，模仿浏览器
如果SSL证书验证不通过，或者操作系统不信任服务器的安全证书，比如浏览器在访问12306网站如：https://www.12306.cn/mormhweb/的时候，会警告用户证书不受信任。

import ssl

# 表示忽略未经核实的SSL证书认证
context = ssl._create_unverified_context()


#  在urlopen()方法里 指明添加 context 参数
response = urllib2.urlopen(request, context = context)
</code></pre>
<p>15、添加新的功能<br>
基本的urlopen()方法不支持代理、Cookie等其他的 HTTP/HTTPS高级功能<br>
使用相关的 Handler处理器 来创建特定功能的处理器对象；</p>
<pre><code>然后通过 urllib2.build_opener()方法使用这些处理器对象，创建自定义opener对象；

使用自定义的opener对象，调用open()方法发送请求。

注意：如果程序里所有的请求都使用自定义的opener，可以使用urllib2.install_opener() 将自定义的 opener 对象 定义为 全局opener，表示如果之后凡是调用urlopen，都将使用这个opener（根据自己的需求来选择）。


15、1 自定义简单的opener

    import urllib2

    # 构建一个HTTPHandler 处理器对象，支持处理HTTP请求
    http_handler = urllib2.HTTPHandler()


    # 调用urllib2.build_opener()方法，创建支持处理HTTP请求的opener对象
    opener = urllib2.build_opener(http_handler)

    # 构建 Request请求
    request = urllib2.Request(&quot;http://www.baidu.com/&quot;)

    # 调用自定义opener对象的open()方法，发送request请求
    # （注意区别：不再通过urllib2.urlopen()发送请求）
    response = opener.open(request)

    # 获取服务器响应内容
    print response.read()


15、2 设置代理器 加代理

    import urllib2

    # 构建了两个代理Handler，一个有代理IP，一个没有代理IP
    httpproxy_handler = urllib2.ProxyHandler({&quot;http&quot; : &quot;168.0.0.1:8100&quot;})
    nullproxy_handler = urllib2.ProxyHandler({})

    proxySwitch = True #定义一个代理开关

    # 通过 urllib2.build_opener()方法使用这些代理Handler对象，创建自定义opener对象
    # 根据代理开关是否打开，使用不同的代理模式
    if proxySwitch:  
        opener = urllib2.build_opener(httpproxy_handler)
    else:
        opener = urllib2.build_opener(nullproxy_handler)

    request = urllib2.Request(&quot;http://www.baidu.com/&quot;)

    # 1. 如果这么写，只有使用opener.open()方法发送请求才使用自定义的代理，而urlopen()则不使用自定义代理。
    response = opener.open(request)

    # 2. 如果这么写，就是将opener应用到全局，之后所有的，不管是opener.open()还是urlopen() 发送请求，都将使用自定义代理。
    # urllib2.install_opener(opener)
    # response = urlopen(request)

    print response.read()
</code></pre>
<p>16、密码管理对象<br>
验证代理授权的用户名和密码 (ProxyBasicAuthHandler())<br>
验证Web客户端的的用户名和密码 (HTTPBasicAuthHandler()) 例如ftp</p>
<p>17、cookie的使用</p>
<pre><code>Cookie：通过在 客户端 记录的信息确定用户的身份。

Session：通过在 服务器端 记录的信息确定用户的身份   


Cookie名字（Name）
Cookie的值（Value）
Cookie的过期时间（Expires/Max-Age）
Cookie作用路径（Path）
Cookie所在域名（Domain），
使用Cookie进行安全连接（Secure）

Set－Cookie: NAME=VALUE；Expires=DATE；Path=PATH；Domain=DOMAIN_NAME；SECURE
</code></pre>
<p>18、cookie在爬虫中的使用</p>
<pre><code>判定注册用户是否已经登录网站，在下一次进入此网站时保留用户信息，可以简化登录或其他验证过程


实例：

    # 获取一个有登录信息的Cookie模拟登陆

    import urllib2

    # 1. 构建一个已经登录过的用户的headers信息
    headers = {
        &quot;Host&quot;:&quot;www.renren.com&quot;,
        &quot;Connection&quot;:&quot;keep-alive&quot;,
        &quot;Upgrade-Insecure-Requests&quot;:&quot;1&quot;,
        &quot;User-Agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;,
        &quot;Accept&quot;:&quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&quot;,
        &quot;Accept-Language&quot;:&quot;zh-CN,zh;q=0.8,en;q=0.6&quot;,

        # 便于终端阅读，表示不支持文件压缩
        # Accept-Encoding: gzip, deflate, sdch,

        # 重点：这个Cookie是一个保存了用户登录状态的Cookie
        &quot;Cookie&quot;:'iehfiowhjqo'}

    # 2. 通过headers里的报头信息（主要是Cookie信息），构建Request对象
    urllib2.Request(&quot;http://www.renren.com/&quot;, headers = headers)

    # 3. 直接访问renren主页，服务器会根据headers报头信息（主要是Cookie信息），判断这是一个已经登录的用户，并返回相应的页面
    response = urllib2.urlopen(request)

    # 4. 打印响应内容
    print response.read()
</code></pre>
<p>19、cookielib  HTTPCookieProcessor</p>
<pre><code>cookielib模块：主要作用是提供用于存储cookie的对象
HTTPCookieProcessor处理器：主要作用是处理这些cookie对象，并构建handler对象 


19、1 获取cookie，保存到cookiejar中


        import urllib2
        import cookielib

        # 构建一个CookieJar对象实例来保存cookie
        cookiejar = cookielib.CookieJar()

        # 使用HTTPCookieProcessor()来创建cookie处理器对象，参数为CookieJar()对象
        handler=urllib2.HTTPCookieProcessor(cookiejar)

        # 通过 build_opener() 来构建opener
        opener = urllib2.build_opener(handler)

        # 4. 以get方法访问页面，访问之后会自动保存cookie到cookiejar中
        opener.open(&quot;http://www.baidu.com&quot;)

        ## 可以按标准格式将保存的Cookie打印出来
        cookieStr = &quot;&quot;
        for item in cookiejar:
            cookieStr = cookieStr + item.name + &quot;=&quot; + item.value + &quot;;&quot;

        ## 舍去最后一位的分号
        print cookieStr[:-1]

19、2 获取cookie，保存到文件中


    import cookielib
    import urllib2

    # 保存cookie的本地磁盘文件名
    filename = 'cookie.txt'

    # 声明一个MozillaCookieJar(有save实现)对象实例来保存cookie，之后写入文件
    cookiejar = cookielib.MozillaCookieJar(filename)

    # 使用HTTPCookieProcessor()来创建cookie处理器对象，参数为CookieJar()对象
    handler = urllib2.HTTPCookieProcessor(cookiejar)

    # 通过 build_opener() 来构建opener
    opener = urllib2.build_opener(handler)

    # 创建一个请求，原理同urllib2的urlopen
    response = opener.open(&quot;http://www.baidu.com&quot;)

    # 保存cookie到本地文件
    cookiejar.save()

19、3 从文件中读取

    import cookielib
    import urllib2

    # 创建MozillaCookieJar(有load实现)实例对象
    cookiejar = cookielib.MozillaCookieJar()

    # 从文件中读取cookie内容到变量
    cookie.load('cookie.txt')

    # 使用HTTPCookieProcessor()来创建cookie处理器对象，参数为CookieJar()对象
    handler = urllib2.HTTPCookieProcessor(cookiejar)

    # 通过 build_opener() 来构建opener
    opener = urllib2.build_opener(handler)

    response = opener.open(&quot;http://www.baidu.com&quot;)
</code></pre>
<p>20、错误<br>
URLError 产生的原因主要有：</p>
<pre><code>没有网络连接
服务器连接失败
找不到指定的服务器


HTTPError
HTTPError是URLError的子类，我们发出一个请求时，服务器上都会对应一个response应答对象，其中它包含一个数字&quot;响应状态码&quot;。

如果urlopen或opener.open不能处理的，会产生一个HTTPError，对应相应的状态码，HTTP状态码表示HTTP协议所返回的响应的状态。
</code></pre>
<p>21、requests包</p>
<pre><code>    http://docs.python-requests.org/zh_CN/latest/index.html

    21、1 最基础的get请求  （headers参数 和 parmas参数）
        response = requests.get(&quot;http://www.baidu.com/&quot;)
        # response = requests.request(&quot;get&quot;, &quot;http://www.baidu.com/&quot;)

        例子：

            import requests

            kw = {'wd':'美食'}

            headers = {&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;}

            # params 接收一个字典或者字符串的查询参数，字典类型自动转换为url编码，不需要urlencode()
            response = requests.get(&quot;http://www.baidu.com/s?&quot;, params = kw, headers = headers)

            # 查看响应内容，response.text 返回的是Unicode格式的数据
            print response.text

            # 查看响应内容，response.content返回的字节流数据
            print respones.content

            # 查看完整url地址
            print response.url

            # 查看响应头部字符编码
            print response.encoding

            # 查看响应码
            print response.status_code



    21.2 post请求

        response = requests.post(&quot;http://www.baidu.com/&quot;, data = data)
        print response.text

        # 如果是json文件可以直接显示
        print response.json()
</code></pre>
<p>22、request 代理服务器</p>
<pre><code># 根据协议类型，选择不同的代理
proxies = {&quot;http&quot;: &quot;http://148.399.56.79:9527&quot;}

response = requests.get(&quot;http://www.baidu.com&quot;, proxies = proxies)
# 私密代理，代表用户名与密码

proxy = { &quot;http&quot;: &quot;xx:123456@148.399.56.79:9527&quot; }

response = requests.get(&quot;http://www.baidu.com&quot;, proxies = proxy)
</code></pre>
<p>23、web客户端验证</p>
<pre><code>auth=('test', '123456')

response = requests.get('http://192.168.34.7', auth = auth)
</code></pre>
<p>24、获取cookie</p>
<pre><code>response = requests.get(&quot;http://www.baidu.com/&quot;)

#  返回CookieJar对象:
cookiejar = response.cookies

#  将CookieJar转为字典：
cookiedict = requests.utils.dict_from_cookiejar(cookiejar)
</code></pre>
<p>25、session<br>
在 requests 里，session对象是一个非常常用的对象，这个对象代表一次用户会话：从客户端浏览器连接服务器开始，到客户端浏览器与服务器断开。</p>
<pre><code>会话能让我们在跨请求时候保持某些参数，比如在同一个 Session 实例发出的所有请求之间保持 cookie 。

data = {&quot;email&quot;:&quot;xx&quot;, &quot;password&quot;:&quot;xx&quot;}  

#  发送附带用户名和密码的请求，并获取登录后的Cookie值，保存在ssion里
ssion.post(&quot;http://www.renren.com/PLogin.do&quot;, data = data)

# ssion包含用户登录后的Cookie值，可以直接访问那些登录后才可以访问的页面
response = ssion.get(&quot;http://www.renren.com/123/profile&quot;)
</code></pre>
<p>26、数字验证证书的问题<br>
跳过 12306 的证书验证，把 verify 设置为 False</p>
<pre><code>r = requests.get(&quot;https://www.12306.cn/mormhweb/&quot;, verify = False)
</code></pre>
<p>27、页面提取指定的内容<br>
非结构化数据：先有数据，再有结构，<br>
结构化数据：先有结构、再有数据</p>
<pre><code>非结构化的数据处理
    文本、电话号码、邮箱地址
    正则表达式
    HTML 文件
    正则表达式
    XPath
    CSS选择器


结构化的数据处理
    JSON 文件
    JSON Path
    转化成Python类型进行操作（json类）
    XML 文件
    转化成Python类型（xmltodict）
    XPath
    CSS选择器
    正则表达式
</code></pre>
<p>28、正则<br>
使用 compile() 函数将正则表达式的字符串形式编译为一个 Pattern 对象</p>
<pre><code>通过 Pattern 对象提供的一系列方法对文本进行匹配查找，获得匹配结果，一个 Match 对象。

最后使用 Match 对象提供的属性和方法获得信息，根据需要进行其他的操作
    
Pattern 对象的一些常用方法主要有：

match 方法：从起始位置开始查找，一次匹配

    match 方法
    match 方法用于查找字符串的头部（也可以指定起始位置），它是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果。它的一般使用形式如下：

    match(string[, pos[, endpos]])
search 方法：从任何位置开始查找，一次匹配

    search 方法
    search 方法用于查找字符串的任何位置，它也是一次匹配，只要找到了一个匹配的结果就返回，而不是查找所有匹配的结果，它的一般使用形式如下：

    search(string[, pos[, endpos]])
findall 方法：全部匹配，返回列表

    indall 方法的使用形式如下：

    findall(string[, pos[, endpos]])

    其中，string 是待匹配的字符串，pos 和 endpos 是可选参数，指定字符串的起始和终点位置，默认值分别是 0 和 len (字符串长度)。

    findall 以列表形式返回全部能匹配的子串，如果没有匹配，则返回一个空列表。
finditer 方法：全部匹配，返回迭代器

    finditer 方法
    finditer 方法的行为跟 findall 的行为类似，也是搜索整个字符串，获得所有匹配的结果。但它返回一个顺序访问每一个匹配结果（Match 对象）的迭代器
split 方法：分割字符串，返回列表


    split 方法
    split 方法按照能够匹配的子串将字符串分割后返回列表，它的使用形式如下：

    split(string[, maxsplit])

    其中，maxsplit 用于指定最大分割次数，不指定将全部分割。
sub 方法：替换
    sub(repl, string[, count])

    repl 可以是字符串也可以是一个函数：

    repl 是字符串，则会使用 repl 去替换字符串每一个匹配的子串，并返回替换后的字符串，另外，repl 还可以使用 id 的形式来引用分组，但不能使用编号 0；

    repl 是函数，这个方法应当只接受一个参数（Match 对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。

    count 用于指定最多替换次数，不指定时全部替换。
</code></pre>
<p>29、匹配中文<br>
import re</p>
<pre><code>title = u'你好，hello，世界'
pattern = re.compile(ur'[\u4e00-\u9fa5]+')
result = pattern.findall(title)

print result
</code></pre>
<p>30、贪婪匹配的问题<br>
注意：贪婪模式与非贪婪模式<br>
贪婪模式：在整个表达式匹配成功的前提下，尽可能多的匹配 ( * )；<br>
非贪婪模式：在整个表达式匹配成功的前提下，尽可能少的匹配 ( ? )；<br>
Python里数量词默认是贪婪的。</p>
<p>31、xml</p>
<pre><code>XML 指可扩展标记语言（EXtensible Markup Language）
XML 是一种标记语言，很类似 HTML
XML 的设计宗旨是传输数据，而非显示数据
XML 的标签需要我们自行定义。
XML 被设计为具有自我描述性。
XML 是 W3C 的推荐标准

数据格式	  描述	                                       设计目标
XML	        Extensible Markup Language （可扩展标记语言）	被设计为传输和存储数据，其焦点是数据的内容。
HTML	    HyperText Markup Language （超文本标记语言）	    显示数据以及如何更好显示数据。
HTML DOM	Document Object Model for HTML (文档对象模型)	   通过 HTML DOM，可以访问所有的 HTML 元素，连同它们所包含的文本和属性。可以对其中的内容进行修改和删除，同时也可以创建新的元素。


XML的节点关系
1. 父（Parent）
每个元素以及属性都有一个父。
2. 子（Children）
元素节点可有零个、一个或多个子。
3. 同胞（Sibling）
拥有相同的父的节点

4. 先辈（Ancestor）
某节点的父、父的父，等等。

5. 后代（Descendant）
某个节点的子，子的子，等等。
</code></pre>
<p>32、xpath</p>
<pre><code>32、1 XPath 开发工具
    开源的XPath表达式编辑工具:XMLQuire(XML格式文件可用)
    Chrome插件 XPath Helper
    Firefox插件 XPath Checker

32、2 选取节点
    表达式	      描述
    nodename	选取此节点的所有子节点。
    /	        从根节点选取。
    //	        从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。
    .	        选取当前节点。
    ..	        选取当前节点的父节点。
    @	        选取属性。

32、3 使用
    from lxml import etree

    html = etree.HTML(text) 

    # 按字符串序列化HTML文档
    result = etree.tostring(html) 
</code></pre>
<p>33、css选择器（配合bs4）<br>
lxml 只会局部遍历，而Beautiful Soup 是基于HTML DOM的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml。<br>
from bs4 import BeautifulSoup</p>
<pre><code>soup = BeautifulSoup(html)
</code></pre>
<p>34、bs转化的四类</p>
<pre><code>Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种:

Tag
HTML 中的一个个标签
NavigableString
BeautifulSoup
Comment
</code></pre>
<p>35、多线程<br>
见案例<br>
多线程：<br>
import requests<br>
import time<br>
from Queue import Queue<br>
from lxml import etree</p>
<pre><code>    import threading

    class Douban(object):
        def __init__(self):
            self.headers = {&quot;User-Agent&quot; : &quot;Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;}
            self.base_url = &quot;https://movie.douban.com/top250?start=&quot;
            self.url_list = [self.base_url + str(page) for page in range(0, 225 + 1, 25)]
            # 创建保存数据的队列
            self.data_queue = Queue()
            self.count = 0

        def send_request(self, url):
            print &quot;[INFO]: 正在抓取&quot; + url
            html = requests.get(url, headers = self.headers).content
            # 每次请求间隔1秒
            time.sleep(1)
            self.parse_page(html)


        def parse_page(self, html):
            html_obj = etree.HTML(html)

            node_list = html_obj.xpath(&quot;//div[@class='info']&quot;)

            for node in node_list:
                # 电影标题
                title = node.xpath(&quot;./div[@class='hd']/a/span[1]/text()&quot;)[0]
                # 电影评分
                score = node.xpath(&quot;.//span[@class='rating_num']/text()&quot;)[0]
                self.count += 1
                self.data_queue.put(score + &quot;\t&quot; + title)


        def start_work(self):

            # 单线程：
            &quot;&quot;&quot;
            for url in self.url_list:
                self.send_request(url)
            &quot;&quot;&quot;

            thread_list = []
            for url in self.url_list:
                # 创建一个线程对象
                thread = threading.Thread(target = self.send_request, args = [url])
                # 启动线程，执行任务
                thread.start()
                # 将当前线程对象存到列表
                thread_list.append(thread)

            # 让主线程等待，所有子线程执行结束，再执行后面的代码
            for thread in thread_list:
                thread.join()


            while not self.data_queue.empty():
                print self.data_queue.get()

            print self.count

    if __name__ == &quot;__main__&quot;:
        douban = Douban()
        start = time.time()
        douban.start_work()

        print &quot;[INFO]: Useing time %f secend&quot; % (time.time() - start)
        # [INFO]: Useing time 1.483035 secend
</code></pre>
<p>36、解决js和jq<br>
Selenium + chromdriver</p>
<pre><code>36、1 鼠标事件
    #导入 ActionChains 类
    from selenium.webdriver import ActionChains

    # 鼠标移动到 ac 位置
    ac = driver.find_element_by_xpath('element')
    ActionChains(driver).move_to_element(ac).perform()


    # 在 ac 位置单击
    ac = driver.find_element_by_xpath(&quot;elementA&quot;)
    ActionChains(driver).move_to_element(ac).click(ac).perform()

    # 在 ac 位置双击
    ac = driver.find_element_by_xpath(&quot;elementB&quot;)
    ActionChains(driver).move_to_element(ac).double_click(ac).perform()

    # 在 ac 位置右击
    ac = driver.find_element_by_xpath(&quot;elementC&quot;)
    ActionChains(driver).move_to_element(ac).context_click(ac).perform()

    # 在 ac 位置左键单击hold住
    ac = driver.find_element_by_xpath('elementF')
    ActionChains(driver).move_to_element(ac).click_and_hold(ac).perform()

    # 将 ac1 拖拽到 ac2 位置
    ac1 = driver.find_element_by_xpath('elementD')
    ac2 = driver.find_element_by_xpath('elementE')
    ActionChains(driver).drag_and_drop(ac1, ac2).perform()

36、2 填充表单

    # 导入 Select 类
    from selenium.webdriver.support.ui import Select

    # 找到 name 的选项卡
    select = Select(driver.find_element_by_name('status'))

    # 
    select.select_by_index(1)
    select.select_by_value(&quot;0&quot;)
    select.select_by_visible_text(u&quot;北京&quot;)
    以上是三种选择下拉框的方式，它可以根据索引来选择，可以根据值来选择，可以根据文字来选择。注意：

    index 索引从 0 开始
    value是option标签的一个属性值，并不是显示在下拉框中的值
    visible_text是在option标签文本的值，是显示在下拉框的值
    全部取消选择怎么办呢？很简单:

    select.deselect_all()

36、3 弹窗处理
    alert = driver.switch_to_alert()
36、4 页面切换
    一个浏览器肯定会有很多窗口，所以我们肯定要有方法来实现窗口的切换。切换窗口的方法如下：

    driver.switch_to.window(&quot;this is window name&quot;)
    也可以使用 window_handles 方法来获取每个窗口的操作对象。例如：

    for handle in driver.window_handles:
        driver.switch_to_window(handle)

36、5 页面前进与后退
    driver.forward()     #前进
    driver.back()        # 后退

36、6 获取cookie
    获取页面每个Cookies值，用法如下

    for cookie in driver.get_cookies():
        print &quot;%s=%s;&quot; % (cookie['name'], cookie['value'])

    除Cookies，用法如下

    # By name
    driver.delete_cookie(&quot;BAIDUID&quot;)

    # all
    driver.delete_all_cookies()

36、7 页面等待
    36、7、1 显示等待
        显式等待
        显式等待指定某个条件，然后设置最长等待时间。如果在这个时间还没有找到元素，那么便会抛出异常了。

        from selenium import webdriver
        from selenium.webdriver.common.by import By
        # WebDriverWait 库，负责循环等待
        from selenium.webdriver.support.ui import WebDriverWait
        # expected_conditions 类，负责条件出发
        from selenium.webdriver.support import expected_conditions as EC

        driver = webdriver.PhantomJS()
        driver.get(&quot;http://www.xxxxx.com/loading&quot;)
        try:
            # 每隔10秒查找页面元素 id=&quot;myDynamicElement&quot;，直到出现则返回
            element = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.ID, &quot;myDynamicElement&quot;))
            )
        finally:
            driver.quit()
        如果不写参数，程序默认会 0.5s 调用一次来查看元素是否已经生成，如果本来元素就是存在的，那么会立即返回。

        下面是一些内置的等待条件，你可以直接调用这些条件，而不用自己写某些等待条件了。

        title_is
        title_contains
        presence_of_element_located
        visibility_of_element_located
        visibility_of
        presence_of_all_elements_located
        text_to_be_present_in_element
        text_to_be_present_in_element_value
        frame_to_be_available_and_switch_to_it
        invisibility_of_element_located
        element_to_be_clickable – it is Displayed and Enabled.
        staleness_of
        element_to_be_selected
        element_located_to_be_selected
        element_selection_state_to_be
        element_located_selection_state_to_be
        alert_is_present
    36、7、2 隐式等待
        from selenium import webdriver

        driver = webdriver.PhantomJS()
        driver.implicitly_wait(10) # seconds
        driver.get(&quot;http://www.xxxxx.com/loading&quot;)
        myDynamicElement = driver.find_element_by_id(&quot;myDynamicElement&quot;)
</code></pre>
<p>37、Scrapy 框架<br>
Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。</p>
<pre><code>Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。

Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，

Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)，

Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.

Downloader Middlewares（下载中间件）：你可以当作是一个可以自定义扩展下载功能的组件。

Spider Middlewares（Spider中间件）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）
</code></pre>
<p>38、安装<br>
sudo pip install scrapy</p>
<p>39、使用<br>
39、1 创建项目<br>
scrapy startproject mySpider</p>
<pre><code>    scrapy.cfg ：项目的配置文件

    mySpider/ ：项目的Python模块，将会从这里引用代码

    mySpider/items.py ：项目的目标文件

    mySpider/pipelines.py ：项目的管道文件

    mySpider/settings.py ：项目的设置文件

    mySpider/spiders/ ：存储爬虫代码目录

39、2 scrapy shell &quot;http://www.baidu.com&quot;

39、3 Selectors选择器
    Scrapy Selectors 内置 XPath 和 CSS Selector 表达式机制
    Selector有四个基本的方法，最常用的还是xpath:

    xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表
    extract(): 序列化该节点为Unicode字符串并返回list
    css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表，语法同 BeautifulSoup4
    re(): 根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表
</code></pre>
<p>40、Scrapy 和 scrapy-redis的区别<br>
Scrapy 是一个通用的爬虫框架，但是不支持分布式，Scrapy-redis是为了更方便地实现Scrapy分布式爬取，而提供了一些以redis为基础的组件(仅有组件)。</p>
<pre><code>pip install scrapy-redis

Scrapy-redis提供了下面四种组件（components）：(四种组件意味着这四个模块都要做相应的修改)

Scheduler
Duplication Filter
Item Pipeline
Base Spider
</code></pre>
<p>41、最后总结一下scrapy-redis的总体思路：<br>
这套组件通过重写scheduler和spider类，实现了调度、spider启动和redis的交互。</p>
<pre><code>实现新的dupefilter和queue类，达到了判重和调度容器和redis的交互，因为每个主机上的爬虫进程都访问同一个redis数据库，所以调度和判重都统一进行统一管理，达到了分布式爬虫的目的。

当spider被初始化时，同时会初始化一个对应的scheduler对象，这个调度器对象通过读取settings，配置好自己的调度容器queue和判重工具dupefilter。

每当一个spider产出一个request的时候，scrapy引擎会把这个reuqest递交给这个spider对应的scheduler对象进行调度，scheduler对象通过访问redis对request进行判重，如果不重复就把他添加进redis中的调度器队列里。当调度条件满足时，scheduler对象就从redis的调度器队列中取出一个request发送给spider，让他爬取。

当spider爬取的所有暂时可用url之后，scheduler发现这个spider对应的redis的调度器队列空了，于是触发信号spider_idle，spider收到这个信号之后，直接连接redis读取strart_url池，拿去新的一批url入口，然后再次重复上边的工作。
</code></pre>
<p>42、https://github.com/rolando/scrapy-redis</p>
<p>43、connection.py<br>
负责根据setting中配置实例化redis连接。被dupefilter和scheduler调用，总之涉及到redis存取的都要使用到这个模块。</p>
<p>44、dupefilter.py<br>
负责执行requst的去重，实现的很有技巧性，使用redis的set数据结构。但是注意scheduler并不使用其中用于在这个模块中实现的dupefilter键做request的调度，而是使用queue.py模块中实现的queue。</p>
<pre><code>当request不重复时，将其存入到queue中，调度时将其弹出。
</code></pre>
<p>45、crapy-Redis分布式策略：<br>
假设有四台电脑：Windows 10、Mac OS X、Ubuntu 16.04、CentOS 7.2，任意一台电脑都可以作为 Master端 或 Slaver端，比如：</p>
<pre><code>Master端(核心服务器) ：使用 Windows 10，搭建一个Redis数据库，不负责爬取，只负责url指纹判重、Request的分配，以及数据的存储

Slaver端(爬虫程序执行端) ：使用 Mac OS X 、Ubuntu 16.04、CentOS 7.2，负责执行爬虫程序，运行过程中提交新的Request给Master
</code></pre>

<span id="footnote"></span>
<div id = "warn"></div>
</div>
<div class="toc-container"></div>
</div>
<div id="fullPage"><canvas id="canvas"></canvas></div>
</article>
<div id="eof"><span>EOF</span></div><div class="round-shape-one"></div>
<section>
<div class="doc_comments">

</div></section>
</div></div></div><script>
"use strict";!function(){for(var n=document.getElementsByTagName("pre"),e=n.length,s=0;s<e;s++){n[s].innerHTML='<span class="line-number"></span>'+n[s].innerHTML+'<span class="cl"></span>';for(var a=n[s].innerHTML.split(/\n/).length,r=0;r<a-1;r++){n[s].getElementsByTagName("span")[0].innerHTML+="<span>"+(r+1)+"</span>"}}}();
let mainNavLinks=document.querySelectorAll(".markdownIt-TOC a");window.addEventListener("scroll",event=>{let fromTop=window.scrollY;mainNavLinks.forEach((link,index)=>{let section=document.getElementById(decodeURI(link.hash).substring(1));let nextSection=null
if(mainNavLinks[index+1]){nextSection=document.getElementById(decodeURI(mainNavLinks[index+1].hash).substring(1));}
if(section.offsetTop<=fromTop){if(nextSection){if(nextSection.offsetTop>fromTop){link.classList.add("currentToc");}else{link.classList.remove("currentToc");}}else{link.classList.add("currentToc");}}else{link.classList.remove("currentToc");}});});
var h=document.documentElement,b=document.body,st="scrollTop",sh="scrollHeight",progress=document.querySelector(".progress"),scroll;document.addEventListener("scroll",function(){scroll=(h[st]||b[st])/((h[sh]||b[sh])-h.clientHeight)*100;progress.style.setProperty("--scroll",scroll+"%")});
var wxScale=new WxScale({fullPage:document.querySelector("#fullPage"),canvas:document.querySelector("#canvas")});var imgBox=document.querySelectorAll("#md_block img");for(var i=0;i<imgBox.length;i++){imgBox[i].onclick=function(e){wxScale.start(this)}};
</script>
<a id="scrollUp" href="#top" style="position: fixed; z-index: 2147483647; display: block;"></a><div class="footer animated fadeInDown"><div class="site_footer"><div class="mysocials"><div class="my_socials"></div></div><div class="copyright"id="copyright">打不死的平头哈Copyright © 2018-2020 <a href="https://18279493170.github.io" style="margin:0;">徐某人</a>.</div>
<span style="display: inline;margin-right:15px;">👁<strong><span id="busuanzi_value_site_uv"></span></strong></span><span id="busuanzi_container_page_pv" style="display: inline;"><span>📚<strong>47</strong> posts</span></div></div>
<script>
console.log("\n %c \u26a1Theme: Bitcron-pro Author's Blog:https://blog.blinkstar.cn  Writen By Serence  \n\n", "color: #ffffff; background: rgba(49, 49, 49, 0.85); padding:5px 0;border-radius:5px;", );
</script>
<script src="https://cdn.jsdelivr.net/npm/instant.page@3.0.0/instantpage.min.js" type="module" defer></script>
<script type="text/javascript" async src="https://18279493170.github.io/media/js/prism.js"></script>
</body>
</html>